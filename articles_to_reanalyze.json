[
  {
    "id": 1,
    "title": "Tom Aarsen (@tomaarsen.com)",
    "url": "https://bsky.app/profile/tomaarsen.com/post/3lsvucbrlpk24",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/tomaarsen.com/post/3lsvucbrlpk24\n\n## Post Content\n\nCould not extract post text from Bluesky\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  },
  {
    "id": 2,
    "title": "Quantization-Aware Training of jina-embeddings-v4",
    "url": "https://jina.ai/news/quantization-aware-training-of-jina-embeddings-v4/",
    "content": "Quantization is a widely used for addressing scaling problems in AI. The name makes it sound complicated, but it\u2019s just rounding numbers off to make them take up less space. This means smaller embedding vectors that take up less memory and storage space, and faster information retrieval because it takes less time to compare vectors. Quantization is a purely numerical technique that doesn\u2019t care what kind of data your model processes or what use cases you have, so it can bring improvements without requiring lots of expensive domain knowledge.\n\nOne might expect that, quantization involves good-old trade-offs and nothing comes for free clich\u00e9 - where we must sacrifice some precision. In this article, we\u2019ll show you a way to **make it lossless** via *quantization-aware training* (QAT). This technique is used in [jina-embeddings-v4](/?sui&model=jina-embeddings-v4) for providing smaller embeddings that required in space-critical applications.\n\n## [*tag*](#overview-of-quantization-techniques \"Overview of Quantization Techniques\")Overview of Quantization Techniques\n\nModel quantization usually means one of four things:\n\n* Post-training quantization (**PTQ**)\n* Training for quantized embedding outputs (**Output QAT**)\n* Training for fully quantized models (**Full QAT**)\n* Distilling a new quantized model from an existing unquantized one\n\nPost-training quantization (**PTQ**) accepts the trained embedding model as is and doesn\u2019t modify it in any way. It\u2019s just a matter of throwing away the least significant digits of the floating point values produced by the model. We just round the numbers off, and sometimes scale them to a range.\n\n**Output QAT** means fine-tuning the embedding model to produce optimal reduced-precision vectors. This means modifying the model, but it doesn\u2019t change the precision of the model\u2019s weights, and therefore doesn\u2019t reduce its size. Just the output vector size is reduced.\n\n**Full QAT** begins with a fully trained, full-precision model and lowers the precision of the model weights, then fine-tunes the performance of this modified model. This produces a significantly smaller model as well as smaller embeddings, at the price of doing some fine-tuning.\n\n**Distillation** is the process of training a new model to match the performance of an existing one. This means creating a new model that\u2019s designed from scratch as quantized, and then using the existing model to generate as much training data as needed to train it until it performs as closely as possible to the existing model.\n\nThe benefits of these four approaches are summarized in the table below:\n\n| Approach | More Compact Embeddings? | Requires Training? | Model Compression? | Faster Inference? |\n| --- | --- | --- | --- | --- |\n| **PTQ** | **\u2713** | \u274c | \u274c | \u274c |\n| **Output QAT** | **\u2713** | **\u2713** | \u274c | \u274c |\n| **Full QAT** | **\u2713** | **\u2713** | **\u2713** | **\u2713** |\n| **Distillation** | | | | |\n| *(to a smaller model)* | **\u2713** | **\u2713** | **\u2713** | **\u2713** |\n\nAll four produce more compact embeddings, but other than PTQ, all require some additional training, while only Full QAT and Distillation produce new, faster models. Full QAT and Distillation are much more expensive to implement because they require a great deal more training than Output QAT.\n\nIn this article, we\u2019re only going to look at PTQ and Output QAT, which don\u2019t change the size or speed of the embedding model.\n\n## [*tag*](#experimental-setup \"Experimental Setup\")Experimental Setup\n\nFor these experiments, our baseline model is [jina-embeddings-v4](/?sui&model=jina-embeddings-v4) with the retrieval adapter, which produces 32-bit-precision floating-point (FP32) vectors in 2048 dimensions. Each embedding is therefore 8196 bytes, or 8kB in size.\n\nWe studied several experimental conditions using query-document retrieval benchmark tasks from the [NanoBEIR benchmark](https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6) suite. The retrieval process uses cosine similarity between vectors to find and rank the documents that best match queries.\n\n* **Baseline** \u2014 The performance of [jina-embeddings-v4](/?sui&model=jina-embeddings-v4) embedding vectors without any quantization. These experiments all used a beta version of the model, and the release performance is somewhat better.\n* **PTQ** \u2014 We quantized the output vectors to binary vectors without changing the model.\n* **Output QAT** \u2014 We quantized the output vectors and applied fine-tuning to the retrieval adapter to improve its performance under quantized conditions.\n\n### [*tag*](#quantization-levels \"Quantization Levels\")Quantization Levels\n\n![Bar chart with a black background showing data sizes in bytes for binary types: 32-bit FP, 16-bit BP, 8-bit int, 4-bit int, a](https://jina-ai-gmbh.ghost.io/content/images/2025/06/image.png)Figure 1: Comparison of post-quantization embedding sizes.We experimented with four different levels of quantization.\n\n* **8-bit integers** \u2014 FP32 values are reduced to integers in the range -128 to 127, shrinking embeddings 4-fold to **2048 bytes**.\n* **4-bit integers** - Same as for 4-bit integers, but we map to the range from -8 to 7, reducing vector sizes by a factor of 8, to **1024 bytes**.\n* **Trinary Quantization \u2014** All values are mapped to one of three values: -1, 0, 1. Optimally stored, this reduces each dimension to 1.6 bits, reducing the size of embedding vectors roughly 40-fold to approximately **230 bytes**.\n* **Binary Quantization** \u2014 We convert FP32 scalar values to one bit, using the `torch.sign` datatype, which provides for just two values, taking one bit to store. This reduces 2048-dimensional embedding vectors from 8192 bytes to **128 bytes**, a 64-fold reduction.\n\n### [*tag*](#scaling \"Scaling\")Scaling\n\nFor binary quantization, quantization is very simple: If a vector value is above 0 or positive, it maps to 1. Otherwise, it maps to -1.\n\n![Blue rectangle on the left and pink rectangle on the right with white arrows indicating movement or interaction, against a bl](https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-1.png)Figure 2: Binary Quantization. All negative values become -1, all others 1.For the other quantization scenarios, we normalized the values to a range and then rounded to the nearest value allowed by the level of quantization. Embedding vectors consist of scale numbers between -\u221e and +\u221e (or, in practice, really big positive and negative numbers). We use two numbers, maxmaxmax and minminmin, to scale the values for quantization.\n\nFor trinary quantization, we take each vector component vvv and translate it as follows:\n\n* if vvv \u2265 maxmaxmax, vvv becomes 1.\n* if vvv \u2264 minminmin, vvv becomes -1.\n* if minminmin < vvv < maxmaxmax, vvv becomes 0.\n\n![A range diagram with blue for minimum, pink for zero, and purple for maximum, arrows indicating smaller values on the left an](https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-2.png)Figure 3: Trinary Quantization. An interval is defined and values within it become 0. All lower values become -1, and all higher ones 1.For 4-bit integers:\n\n* if vvv \u2265 maxmaxmax, vvv becomes 7.\n* if vvv \u2264 minminmin, vvv becomes -8.\n* if minminmin < vvv < maxmaxmax, vvv becomes 16\u2217(v\u2212min)/(max\u2212min)\u2212816\\*(v - min)/(max - min) - 816\u2217(v\u2212min)/(max\u2212min)\u22128, then rounded to the nearest integer. This scales the value to the range [\u22128,7][-8,7][\u22128,7].\n\n![Abstract diagram featuring scales with numbers ranging from 8 to 7, accompanied by directional arrows, suggesting symmetry an](https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-3.png)Figure 4: 4-bit Quantization. An interval is defined and all values are normalized to the defined range [-8,7].For 8-bit integers:\n\n* if vvv \u2265 maxmaxmax, vvv becomes 127.\n* if vvv \u2264 minminmin, vvv becomes -128.\n* if minminmin < vvv < maxmaxmax, vvv becomes 256\u2217(v\u2212min)/(max\u2212min)\u2212128256\\*(v - min)/(max - min) - 128256\u2217(v\u2212min)/(max\u2212min)\u2212128, rounded to the nearest integer. This scales the value to the range [\u2212128,127][-128,127][\u2212128,127].\n\n![Horizontal bar with a gradient transitioning from blue to magenta to pink, divided by black sections, flanked by white arrows](https://jina-ai-gmbh.ghost.io/content/images/2025/06/image-4.png)Figure 5: 8-bit Quantization. An interval is defined and all values are normalized to the defined range [-128,127].To calculate maxmaxmax and minminmin, we used two approaches:\n\n* **Min/Max** \u2014 We processed our data in batches, and for each batch, we identified the highest and lowest vector component, setting maxmaxmax to the highest and minminmin to the lowest.\n* **Rolling averaging over batches** \u2014 For each batch, we calculated the average and standard deviation of the vector components. We maintained a moving average of both the average and standard deviation as we processed all batches. If avgavgavg is the current moving average of batch average values, and stdstdstd is the current moving average of the standard deviations, then for each batch:\n\nmax=avg+stdmax = avg + stdmax=avg+std \nmin=avg\u2212stdmin = avg - stdmin=avg\u2212std\n\n### [*tag*](#qat-fine-tuning \"QAT Fine-Tuning\")QAT Fine-Tuning\n\nFor the PTQ experiments, we used the model as is and quantized the embeddings it produced using the methods described above.\n\nFor the Output QAT, we fine-tuned the model using *straight-through estimation*. This means that we reverse the quantization process, restoring the full precision to the values, before calculating the loss (i.e., error), and then we use that loss metric to fine-tune the model.\n\nWe fine-tuned in each case for 10,000 steps, saving a checkpoint every 500 steps. We then retained the checkpoint with the highest score on the [NanoBEIR](https://huggingface.co/collections/zeta-alpha-ai/nanobeir-66e1a0af21dfd93e620cd9f6) benchmark.\n\n### [*tag*](#asymmetric-quantization \"Asymmetric Quantization\")Asymmetric Quantization\n\nPTQ and Output QAT reduce the size of the embedding vectors, but don\u2019t reduce model size or inference speed; all the savings are in the size of the stored document embeddings and retrieval speed.\n\nAs a result, we tested both quantizing the query vectors and leaving them unquantized at retrieval time because it doesn\u2019t change the size of the stored embedding vectors either way.\n\n## [*tag*](#results \"Results\")Results\n\nWe tested nine conditions in total, summarized in the tables below:\n\n| Condition Name | Fine-Tuning | Quantization Level | Scaling Strategy | Quantized Queries |\n| --- | --- | --- | --- | --- |\n| Baseline | \u274c | n/a | n/a | n/a |\n| PTQ Both | \u274c | Binary | n/a | **\u2713** |\n| PTQ Docs Only | \u274c | Binary | n/a | \u274c |\n| QAT Binary | **\u2713** | Binary | n/a | **\u2713** |\n| QAT Binary Docs Only | **\u2713** | Binary | n/a | \u274c |\n| QAT Trinary | **\u2713** | Trinary | Rolling Average | **\u2713** |\n| QAT 4-bits | **\u2713** | 4-bits | Rolling Average | **\u2713** |\n| QAT 8-bits | **\u2713** | 8-bits | Rolling Average | **\u2713** |\n| QAT 8-bits Min/Max | **\u2713** | 8-bits | Min/Max | **\u2713** |\n\n*Table 2: Experimental Conditions*\n\n| Condition Name | Average Score | Difference from baseline |\n| --- | --- | --- |\n| Baseline | 60.10 | n/a |\n| PTQ Binary | 58.33 | -1.78 |\n| PTQ Binary Docs Only | 59.08 | -1.02 |\n| QAT Binary | 59.22 | -0.89 |\n| QAT Binary Docs Only | 60.81 | +0.70 |\n| QAT Trinary | 59.49 | -0.62 |\n| QAT 4-bits | 61.73 | +1.62 |\n| QAT 8-bits | 61.67 | +1.56 |\n| QAT 8-bits Min/Max | 61.29 | +1.19 |\n\n*Table 3: Average score (in % correct) for each condition over the twelve NanoBEIR benchmarks.*\n\nYou can see from the table above that fine-tuning for quantization improves scores. The only difference between the **PTQ Binary** and **QAT Binary** conditions is fine-tuning, and the difference in score is significant. Similarly, we see an almost 2% improvement in scores between the **PTQ Binary Docs Only** and **QAT Binary Docs Only** conditions, which are only distinguished by the same fine-tuning.\n\nUnsurprisingly, we also see that scores generally improve the less we quantize, with 4-bit quantization scoring better than trinary, and trinary better than binary. However, going further to 8-bits doesn\u2019t appear to have improved anything.\n\nWe only tested leaving queries unquantized in binary cases, but this appears to improve performance.\n\nFinally, our tests suggest that the rolling average scaling method outperforms the simplistic min/max approach.\n\n## [*tag*](#conclusion \"Conclusion\")Conclusion\n\nQuantization has some important operational advantages for embedding models, by significantly reducing the size of embedding vectors and accelerating information retrieval. While simple post-training quantization (PTQ) provides immediate benefits in terms of memory and storage, our experiments demonstrate that quantization-aware training (QAT) significantly mitigates the inevitable precision losses. Fine-tuning consistently yielded better scores.\n\nThe degree of quantization directly impacts performance, which is what you would expect from a method based on reducing the precision of values. Less aggressive quantization (e.g., 4-bit) generally outperforms more aggressive methods (e.g., binary), but surprisingly, there was no significant difference in performance between 8-bit and 4-bit quantization. It would seem that until you reach some threshold of imprecision, there is little difference between greater and lesser quantization.\n\nScaling strategies are also significant, with the rolling average method showing superior results compared to a fixed min/max approach. Using scaling values that are relative to the data appears to work significantly better and merits further exploration.\n\nQuantization can get you more out of your embedding models for less. Although this article doesn\u2019t explore all the options for quantization, it explores two that are easily accessible, and they have real benefits to offer. We\u2019re working to refine and improve quantization strategies so that we can further reduce users' costs, and expect to release binary support for [jina-embeddings-v4](/?sui&model=jina-embeddings-v4) in the near future."
  },
  {
    "id": 3,
    "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
    "url": "https://arxiv.org/abs/2506.16655",
    "content": "# Arch-Router: Aligning LLM Routing with Human Preferences\n\n**Authors:** Authors:Co Tran, Salman Paracha, Adil Hafeez, Shuguang Chen\n\n**Subjects:** Computation and Language (cs.CL)\n\n## Abstract\n\nWith the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models. In this work, we propose a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce \\textbf{Arch-Router}, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Our approach also supports seamlessly adding new models for routing without requiring retraining or architectural modifications. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models. Our approach captures subjective evaluation criteria and makes routing decisions more transparent and flexible. Our model is available at: \\texttt{this https URL}.\n\n"
  },
  {
    "id": 4,
    "title": "Text-to-LoRA: Instant Transformer Adaption",
    "url": "https://arxiv.org/abs/2506.06105",
    "content": "# Text-to-LoRA: Instant Transformer Adaption\n\n**Authors:** Authors:Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Tjarko Lange\n\n**Subjects:** Machine Learning (cs.LG); Artificial Intelligence (cs.AI)\n\n## Abstract\n\nWhile Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements.\nOur code is available at this https URL\n\n"
  },
  {
    "id": 5,
    "title": "Sumit (@reachsumit.com)",
    "url": "https://bsky.app/profile/reachsumit.com/post/3lssbir3mk222",
    "content": "# IRanker: Towards Ranking Foundation Model\n\n**Shared via Bluesky:** https://bsky.app/profile/reachsumit.com/post/3lssbir3mk222\n**Original Paper:** https://arxiv.org/abs/2506.21638\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\nSumit\nreachsumit.com\ndid:plc:iakjx7d3ypfa5ycegrylvt2m\nIRanker: Towards Ranking Foundation Model\n\nIntroduces a ranking foundation model with reinforcement learning and iterative decoding that decomposes complex ranking tasks into step-by-step candidate elimination.\n\n\ud83d\udcdd arxiv.org/abs/2506.21638\n\ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb https://github.com/ulab-uiuc/IRanker\nhttps://arxiv.org/abs/2506.21638\n2025-06-30T03:45:55.336Z\n\n---\n\n# IRanker: Towards Ranking Foundation Model\n\n**Authors:** Authors:Tao Feng, Zhigang Hua, Zijie Lei, Yan Xie, Shuang Yang, Bo Long, Jiaxuan You\n\n**Subjects:** Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\n\n## Abstract\n\nRanking tasks are ubiquitous, encompassing applications such as recommendation systems, LLM routing, and item re-ranking. We propose to unify these tasks using a single ranking foundation model (FM), as it eliminates the need for designing different models for each specific ranking task. However, unlike general supervision tasks in LLMs, ranking tasks do not have clear labels for supervision, posing great challenges to developing a ranking FM. To overcome these challenges, we propose IRanker, a ranking FM framework with reinforcement learning (RL) and iterative decoding. Our insight is to decompose the complex ranking task into an iterative decoding process that eliminates the worst candidate from the candidate pool step by step, which significantly reduces the output combinatorial space and better utilizes the limited context length during RL training. We meticulously train and comprehensively evaluate an IRanker-3B model on nine datasets across three scenarios: recommendation, routing, and passage ranking. The results show that a single IRanker-3B achieves state-of-the-art results on several datasets compared to models of similar size, and even surpasses the performance of larger models on certain datasets. We further demonstrate the effectiveness of our RL design and the robustness of the iterative mechanism across different LLM sizes. Moreover, we conducted both in-domain and out-of-domain zero-shot generalization experiments, which showed that IRanker-3B achieved good generalization on in-domain ranking tasks compared to the base LLM by at least 5% improvement. Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the thoughts generated by IRanker-3B during training could further enhance zero-shot LLM performance.\n\n"
  },
  {
    "id": 6,
    "title": "Sumit (@reachsumit.com)",
    "url": "https://bsky.app/profile/reachsumit.com/post/3lssbxtzylc22",
    "content": "# VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation\n\n**Shared via Bluesky:** https://bsky.app/profile/reachsumit.com/post/3lssbxtzylc22\n**Original Paper:** https://arxiv.org/abs/2506.21556\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\nSumit\nreachsumit.com\ndid:plc:iakjx7d3ypfa5ycegrylvt2m\nVAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation\n\nIntroduces a concept-centric multimodal knowledge graph covering visual, audio, and text modalities with detailed descriptions.\n\n\ud83d\udcdd arxiv.org/abs/2506.21556\n\ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb vatkg.github.io\nhttps://arxiv.org/abs/2506.21556\n2025-06-30T03:54:22.437Z\n\n---\n\n# VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation\n\n**Authors:** Authors:Hyeongcheol Park, MinHyuk Jang, Ha Dam Baek, Gyusam Chang, Jiyoung Seo, Jiwan Park, Hogun Park, Sangpil Kim\n\n**Subjects:** Computation and Language (cs.CL)\n\n## Abstract\n\nMultimodal Knowledge Graphs (MMKGs), which represent explicit knowledge across multiple modalities, play a pivotal role by complementing the implicit knowledge of Multimodal Large Language Models (MLLMs) and enabling more grounded reasoning via Retrieval Augmented Generation (RAG). However, existing MMKGs are generally limited in scope: they are often constructed by augmenting pre-existing knowledge graphs, which restricts their knowledge, resulting in outdated or incomplete knowledge coverage, and they often support only a narrow range of modalities, such as text and visual information. These limitations reduce their extensibility and applicability to a broad range of multimodal tasks, particularly as the field shifts toward richer modalities such as video and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive multimodal knowledge graph that covers visual, audio, and text information, where each triplet is linked to multimodal data and enriched with detailed descriptions of concepts. Specifically, our construction pipeline ensures cross-modal knowledge alignment between multimodal data and fine-grained semantics through a series of stringent filtering and alignment steps, enabling the automatic generation of MMKGs from any multimodal dataset. We further introduce a novel multimodal RAG framework that retrieves detailed concept-level knowledge in response to queries from arbitrary modalities. Experiments on question answering tasks across various modalities demonstrate the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical value in unifying and leveraging multimodal knowledge.\n\n"
  },
  {
    "id": 7,
    "title": "arxiv cs.IR (@arxiv-cs-ir.bsky.social)",
    "url": "https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lssft2zuof25",
    "content": "# ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation\n\n**Shared via Bluesky:** https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lssft2zuof25\n**Original Paper:** https://arxiv.org/abs/2506.21931\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\narxiv cs.IR\narxiv-cs-ir.bsky.social\ndid:plc:aq67jotdjmceysktj6nq6gqy\nReza Yousefi Maragheh, Pratheek Vadla, Priyank Gupta, Kai Zhao, Aysenur Inan, Kehui Yao, Jianpeng Xu, Praveen Kanumala, Jason Cho, Sushant Kumar\nARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation\nhttps://arxiv.org/abs/2506.21931\n2025-06-30T05:03:16.338Z\n\n---\n\n# ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation\n\n**Authors:** Authors:Reza Yousefi Maragheh, Pratheek Vadla, Priyank Gupta, Kai Zhao, Aysenur Inan, Kehui Yao, Jianpeng Xu, Praveen Kanumala, Jason Cho, Sushant Kumar\n\n**Subjects:** Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)\n\n## Abstract\n\nRetrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.\n\n"
  },
  {
    "id": 8,
    "title": "arxiv cs.IR (@arxiv-cs-ir.bsky.social)",
    "url": "https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lssineizm42c",
    "content": "# Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization\n\n**Shared via Bluesky:** https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lssineizm42c\n**Original Paper:** https://arxiv.org/abs/2506.21601\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\narxiv cs.IR\narxiv-cs-ir.bsky.social\ndid:plc:aq67jotdjmceysktj6nq6gqy\nDuong Bach\nHierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization\nhttps://arxiv.org/abs/2506.21601\n2025-06-30T05:53:46.138Z\n\n---\n\n# Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization\n\n**Authors:** Authors:Duong Bach\n\n**Subjects:** Information Retrieval (cs.IR); Computer Vision and Pattern Recognition (cs.CV)\n\n## Abstract\n\nMulti-vector document retrieval systems, such as ColPali, excel in fine-grained matching for complex queries but incur significant storage and computational costs due to their reliance on high-dimensional patch embeddings and late-interaction scoring. To address these challenges, we propose HPC-ColPali, a Hierarchical Patch Compression framework that enhances the efficiency of ColPali while preserving its retrieval accuracy. Our approach integrates three innovative techniques: (1) K-Means quantization, which compresses patch embeddings into 1-byte centroid indices, achieving up to 32$\\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing Vision-Language Model attention weights to retain only the top-$p\\%$ most salient patches, reducing late-interaction computation by up to 60\\% with less than 2\\% nDCG@10 loss; and (3) optional binary encoding of centroid indices into $b$-bit strings ($b=\\lceil\\log_2 K\\rceil$), enabling rapid Hamming distance-based similarity search for resource-constrained environments. Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\\% lower query latency under HNSW indexing while maintaining high retrieval precision. When integrated into a Retrieval-Augmented Generation pipeline for legal summarization, it reduces hallucination rates by 30\\% and halves end-to-end latency. These advancements establish HPC-ColPali as a scalable and efficient solution for multi-vector document retrieval across diverse applications. Code is available at this https URL.\n\n"
  },
  {
    "id": 9,
    "title": "arxiv cs.IR (@arxiv-cs-ir.bsky.social)",
    "url": "https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lssiq54mri2x",
    "content": "# PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications\n\n**Shared via Bluesky:** https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lssiq54mri2x\n**Original Paper:** https://arxiv.org/abs/2506.21593\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\narxiv cs.IR\narxiv-cs-ir.bsky.social\ndid:plc:aq67jotdjmceysktj6nq6gqy\nAbu Hanif Muhammad Syarubany, Chang Dong Yoo\nPentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications\nhttps://arxiv.org/abs/2506.21593\n2025-06-30T05:55:19.039Z\n\n---\n\n# PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications\n\n**Authors:** Authors:Abu Hanif Muhammad Syarubany, Chang Dong Yoo\n\n**Subjects:** Information Retrieval (cs.IR); Databases (cs.DB)\n\n## Abstract\n\nEnterprise deployments of large-language model (LLM) demand continuously changing document collections with sub-second latency and predictable GPU cost requirements that classical Retrieval-Augmented Generation (RAG) pipelines only partially satisfy. We present PentaRAG, a five-layer module that routes each query through two instant caches (fixed key-value and semantic), a memory-recall mode that exploits the LLM's own weights, an adaptive session memory, and a conventional retrieval-augmentation layer. Implemented with Mistral-8B, Milvus and vLLM, the system can answer most repeated or semantically similar questions from low-latency caches while retaining full retrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined with the memory-recall layer raises answer similarity by approximately 8% and factual correctness by approximately 16% over the base model. Under a nine-session runtime simulation, cache warming reduces mean latency from several seconds to well below one second and shifts traffic toward the fast paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to 0.248 seconds per query, roughly half that of a naive RAG baseline, and sustains an aggregate throughput of approximately 100,000 queries per second on our setup. These results demonstrate that a layered routing strategy can deliver freshness, speed, and efficiency simultaneously in production-grade RAG systems.\n\n"
  },
  {
    "id": 10,
    "title": "arxiv cs.IR (@arxiv-cs-ir.bsky.social)",
    "url": "https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lsskaxcsh52p",
    "content": "# LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation\n\n**Shared via Bluesky:** https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lsskaxcsh52p\n**Original Paper:** https://arxiv.org/abs/2506.21579\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\narxiv cs.IR\narxiv-cs-ir.bsky.social\ndid:plc:aq67jotdjmceysktj6nq6gqy\nYingzhi He, Xiaohao Liu, An Zhang, Yunshan Ma, Tat-Seng Chua\nLLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation\nhttps://arxiv.org/abs/2506.21579\n2025-06-30T06:22:37.240Z\n\n---\n\n# LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation\n\n**Authors:** Authors:Yingzhi He, Xiaohao Liu, An Zhang, Yunshan Ma, Tat-Seng Chua\n\n**Subjects:** Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)\n\n## Abstract\n\nSequential recommendation aims to predict users' future interactions by modeling collaborative filtering (CF) signals from historical behaviors of similar users or items. Traditional sequential recommenders predominantly rely on ID-based embeddings, which capture CF signals through high-order co-occurrence patterns. However, these embeddings depend solely on past interactions, lacking transferable knowledge to generalize to unseen domains. Recent advances in large language models (LLMs) have motivated text-based recommendation approaches that derive item representations from textual descriptions. While these methods enhance generalization, they fail to encode CF signals-i.e., latent item correlations and preference patterns-crucial for effective recommendation. We argue that an ideal embedding model should seamlessly integrate CF signals with rich semantic representations to improve both in-domain and out-of-domain recommendation performance.\nTo this end, we propose LLM2Rec, a novel embedding model tailored for sequential recommendation, integrating the rich semantic understanding of LLMs with CF awareness. Our approach follows a two-stage training framework: (1) Collaborative Supervised Fine-tuning, which adapts LLMs to infer item relationships based on historical interactions, and (2) Item-level Embedding Modeling, which refines these specialized LLMs into structured item embedding models that encode both semantic and collaborative information. Extensive experiments on real-world datasets demonstrate that LLM2Rec effectively improves recommendation quality across both in-domain and out-of-domain settings. Our findings highlight the potential of leveraging LLMs to build more robust, generalizable embedding models for sequential recommendation. Our codes are available at this https URL.\n\n"
  },
  {
    "id": 11,
    "title": "Paper (@paper.bsky.social)",
    "url": "https://bsky.app/profile/paper.bsky.social/post/3lshtglohzr2d",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/paper.bsky.social/post/3lshtglohzr2d\n\n## Post Content\n\nPost\nPaper\npaper.bsky.social\ndid:plc:z5xxhxqv6elnjzulyf7t22wk\n[29/30] 205 Likes, 11 Comments, 4 Posts\n2506.06105, cs\u2024LG | cs\u2024AI, 09 Jun 2025\n\n\ud83c\udd95Text-to-LoRA: Instant Transformer Adaption\n\nRujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Tjarko Lange\n2025-06-26T00:07:33.543Z\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  },
  {
    "id": 12,
    "title": "Sumit (@reachsumit.com)",
    "url": "https://bsky.app/profile/reachsumit.com/post/3lsi5qzveoc2x",
    "content": "# Controlled Retrieval-augmented Context Evaluation for Long-form RAG\n\n**Shared via Bluesky:** https://bsky.app/profile/reachsumit.com/post/3lsi5qzveoc2x\n**Original Paper:** https://arxiv.org/abs/2506.20051\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\nSumit\nreachsumit.com\ndid:plc:iakjx7d3ypfa5ycegrylvt2m\nControlled Retrieval-augmented Context Evaluation for Long-form RAG\n\nIntroduces a framework for evaluating retrieval context in long-form RAG using human-written summaries to control information scope.\n\n\ud83d\udcdd arxiv.org/abs/2506.20051\n\ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb https://anonymous.4open.science/r/rag-rerank-85CF\nhttps://arxiv.org/abs/2506.20051\n2025-06-26T03:12:17.443Z\n\n---\n\n# Controlled Retrieval-augmented Context Evaluation for Long-form RAG\n\n**Authors:** Authors:Jia-Huei Ju, Suzan Verberne, Maarten de Rijke, Andrew Yates\n\n**Subjects:** Information Retrieval (cs.IR)\n\n## Abstract\n\nRetrieval-augmented generation (RAG) enhances large language models by incorporating context retrieved from external knowledge sources. While the effectiveness of the retrieval module is typically evaluated with relevance-based ranking metrics, such metrics may be insufficient to reflect the retrieval's impact on the final RAG result, especially in long-form generation scenarios. We argue that providing a comprehensive retrieval-augmented context is important for long-form RAG tasks like report generation and propose metrics for assessing the context independent of generation. We introduce CRUX, a \\textbf{C}ontrolled \\textbf{R}etrieval-a\\textbf{U}gmented conte\\textbf{X}t evaluation framework designed to directly assess retrieval-augmented contexts. This framework uses human-written summaries to control the information scope of knowledge, enabling us to measure how well the context covers information essential for long-form generation. CRUX uses question-based evaluation to assess RAG's retrieval in a fine-grained manner. Empirical results show that CRUX offers more reflective and diagnostic evaluation. Our findings also reveal substantial room for improvement in current retrieval methods, pointing to promising directions for advancing RAG's retrieval. Our data and code are publicly available to support and advance future research on retrieval.\n\n"
  },
  {
    "id": 13,
    "title": "Sung Kim (@sungkim.bsky.social)",
    "url": "https://bsky.app/profile/sungkim.bsky.social/post/3lrs76hb3tk2p",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/sungkim.bsky.social/post/3lrs76hb3tk2p\n\n## Post Content\n\nPost\nSung Kim\nsungkim.bsky.social\ndid:plc:cq4gg3odxz2pzmkx2fuac3u3\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\n\nThey analyzed more than 80 commercial and non-commercial implementations that have emerged since 2023, including OpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research,\n2025-06-17T09:39:14.148Z\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  },
  {
    "id": 14,
    "title": "Sung Kim (@sungkim.bsky.social)",
    "url": "https://bsky.app/profile/sungkim.bsky.social/post/3lrlxhzbtsk26",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/sungkim.bsky.social/post/3lrlxhzbtsk26\n\n## Post Content\n\nPost\nSung Kim\nsungkim.bsky.social\ndid:plc:cq4gg3odxz2pzmkx2fuac3u3\nThey advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agents.\n\n\"Build the web for agents, not agents for the web\"\n\narxiv.org/abs/2506.10953\n2025-06-14T22:05:25.347Z\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  },
  {
    "id": 15,
    "title": "LlamaIndex (@llamaindex.bsky.social)",
    "url": "https://bsky.app/profile/llamaindex.bsky.social/post/3lt35nmxess2v",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/llamaindex.bsky.social/post/3lt35nmxess2v\n\n## Post Content\n\nCould not extract post text from Bluesky\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  },
  {
    "id": 16,
    "title": "Gl\u00f3rIA: A Generative and Open Large Language Model for Portuguese Pre-print - Accepted for publication at PROPOR 2024.",
    "url": "https://arxiv.org/html/2402.12969v1",
    "content": "# \n\n"
  },
  {
    "id": 17,
    "title": "Sung Kim (@sungkim.bsky.social)",
    "url": "https://bsky.app/profile/sungkim.bsky.social/post/3lt35yhxylc27",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/sungkim.bsky.social/post/3lt35yhxylc27\n\n## Post Content\n\nCould not extract post text from Bluesky\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  },
  {
    "id": 19,
    "title": "LangChain (@langchain.bsky.social)",
    "url": "https://bsky.app/profile/langchain.bsky.social/post/3lsyxf2dshk2q",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/langchain.bsky.social/post/3lsyxf2dshk2q\n\n## Post Content\n\nCould not extract post text from Bluesky\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  },
  {
    "id": 20,
    "title": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble",
    "url": "https://arxiv.org/abs/2502.18036",
    "content": "# Harnessing Multiple Large Language Models: A Survey on LLM Ensemble\n\n**Authors:** Authors:Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu\n\n**Subjects:** Computation and Language (cs.CL)\n\n## Abstract\n\nLLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. This paper presents the first systematic review of recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. Then, we provide a more in-depth classification of the methods under the broad categories of \"ensemble-before-inference, ensemble-during-inference, ensemble-after-inference'', and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. A curated list of papers on LLM Ensemble is available at this https URL.\n\n"
  },
  {
    "id": 21,
    "title": "Context Engineering",
    "url": "https://blog.langchain.com/context-engineering-for-agents/",
    "content": "### TL;DR\n\nAgents need context to perform tasks. Context engineering is the art and science of filling the context window with just the right information at each step of an agent\u2019s trajectory. In this post, we break down some common strategies \u2014 **write, select, compress, and isolate \u2014** for context engineering by reviewing various popular agents and papers. We then explain how LangGraph is designed to support them! \n\n**Also, see our video on context engineering** [**here**](https://youtu.be/4GiqzUHD5AA?ref=blog.langchain.com)**.**\n\n!General categories of context engineering### Context Engineering\n\nAs Andrej Karpathy puts it, LLMs are like a [new kind of operating system](https://www.youtube.com/watch?si=-aKY-x57ILAmWTdw&t=620&v=LCEmiRjPEtQ&feature=youtu.be&ref=blog.langchain.com). The LLM is like the CPU and its [context window](https://docs.anthropic.com/en/docs/build-with-claude/context-windows?ref=blog.langchain.com) is like the RAM, serving as the model\u2019s working memory. Just like RAM, the LLM context window has limited [capacity](https://lilianweng.github.io/posts/2023-06-23-agent/?ref=blog.langchain.com) to handle various sources of context. And just as an operating system curates what fits into a CPU\u2019s RAM, we can think about \u201ccontext engineering\u201d playing a similar role. [Karpathy summarizes this well](https://x.com/karpathy/status/1937902205765607626?ref=blog.langchain.com):\n\n> *[Context engineering is the] \u201d\u2026delicate art and science of filling the context window with just the right information for the next step.\u201d*\n\n!Context types commonly used in LLM applicationsWhat are the types of context that we need to manage when building LLM applications? Context engineering as an [umbrella](https://x.com/dexhorthy/status/1933283008863482067?ref=blog.langchain.com) that applies across a few different context types:\n\n* **Instructions** \u2013 prompts, memories, few\u2011shot examples, tool descriptions, etc\n* **Knowledge** \u2013 facts, memories, etc\n* **Tools** \u2013 feedback from tool calls\n\n### Context Engineering for Agents\n\nThis year, interest in [agents](https://www.anthropic.com/engineering/building-effective-agents?ref=blog.langchain.com) has grown tremendously as LLMs get better at [reasoning](https://platform.openai.com/docs/guides/reasoning?api-mode=responses&ref=blog.langchain.com) and [tool calling](https://www.anthropic.com/engineering/building-effective-agents?ref=blog.langchain.com). [Agents](https://www.anthropic.com/engineering/building-effective-agents?ref=blog.langchain.com) interleave [LLM invocations and tool calls](https://www.anthropic.com/engineering/building-effective-agents?ref=blog.langchain.com), often for [long-running tasks](https://blog.langchain.com/introducing-ambient-agents/). Agents interleave [LLM calls and tool calls](https://www.anthropic.com/engineering/building-effective-agents?ref=blog.langchain.com), using tool feedback to decide the next step.\n\n!Agents interleave [LLM calls and](https://www.anthropic.com/engineering/building-effective-agents?ref=blog.langchain.com) [tool calls](https://www.anthropic.com/engineering/building-effective-agents?ref=blog.langchain.com), using tool feedback to decide the next stepHowever, long-running tasks and accumulating feedback from tool calls mean that agents often utilize a large number of tokens. This can cause numerous problems: it can [exceed the size of the context window](https://cognition.ai/blog/kevin-32b?ref=blog.langchain.com), balloon cost / latency, or degrade agent performance. Drew Breunig [nicely outlined](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com) a number of specific ways that longer context can cause perform problems, including:\n\n* [Context Poisoning: When a hallucination makes it into the context](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-poisoning)\n* [Context Distraction: When the context overwhelms the training](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-distraction)\n* [Context Confusion: When superfluous context influences the response](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-confusion)\n* [Context Clash: When parts of the context disagree](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html?ref=blog.langchain.com#context-clash)\n\n!Context from tool calls accumulates over multiple agent turnsWith this in mind, [Cognition](https://cognition.ai/blog/dont-build-multi-agents?ref=blog.langchain.com) called out the importance of context engineering:\n\n> *\u201cContext engineering\u201d \u2026 is effectively the #1 job of engineers building AI agents.*\n\n[Anthropic](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com) also laid it out clearly:\n\n> *Agents often engage in conversations spanning hundreds of turns, requiring careful context management strategies.*\n\nSo, how are people tackling this challenge today? We group common strategies for agent context engineering into four buckets \u2014 **write, select, compress, and isolate \u2014** and give examples of each from review of some popular agent products and papers. We then explain how LangGraph is designed to support them!\n\n!General categories of context engineering### Write Context\n\n*Writing context means saving it outside the context window to help an agent perform a task.*\n\n**Scratchpads**\n\nWhen humans solve tasks, we take notes and remember things for future, related tasks. Agents are also gaining these capabilities! Note-taking via a \u201c[scratchpad](https://www.anthropic.com/engineering/claude-think-tool?ref=blog.langchain.com)\u201d is one approach to persist information while an agent is performing a task. The idea is to save information outside of the context window so that it\u2019s available to the agent. [Anthropic\u2019s multi-agent researcher](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com) illustrates a clear example of this:\n\n> *The LeadResearcher begins by thinking through the approach and saving its plan to Memory to persist the context, since if the context window exceeds 200,000 tokens it will be truncated and it is important to retain the plan.*\n\nScratchpads can be implemented in a few different ways. They can be a [tool call](https://www.anthropic.com/engineering/claude-think-tool?ref=blog.langchain.com) that simply [writes to a file](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem?ref=blog.langchain.com). They can also be a field in a runtime [state object](https://langchain-ai.github.io/langgraph/concepts/low_level/?ref=blog.langchain.com#state) that persists during the session. In either case, scratchpads let agents save useful information to help them accomplish a task.\n\n**Memories**\n\nScratchpads help agents solve a task within a given session (or [thread](https://langchain-ai.github.io/langgraph/concepts/persistence/?ref=blog.langchain.com#threads)), but sometimes agents benefit from remembering things across *many* sessions! [Reflexion](https://arxiv.org/abs/2303.11366?ref=blog.langchain.com) introduced the idea of reflection following each agent turn and re-using these self-generated memories. [Generative Agents](https://ar5iv.labs.arxiv.org/html/2304.03442?ref=blog.langchain.com) created memories synthesized periodically from collections of past agent feedback.\n\n!An LLM can be used to update or create memoriesThese concepts made their way into popular products like [ChatGPT](https://help.openai.com/en/articles/8590148-memory-faq?ref=blog.langchain.com), [Cursor](https://forum.cursor.com/t/0-51-memories-feature/98509?ref=blog.langchain.com), and [Windsurf](https://docs.windsurf.com/windsurf/cascade/memories?ref=blog.langchain.com), which all have mechanisms to auto-generate long-term memories that can persist across sessions based on user-agent interactions.\n\n### Select Context\n\n*Selecting context means pulling it into the context window to help an agent perform a task.*\n\n**Scratchpad**\n\nThe mechanism for selecting context from a scratchpad depends upon how the scratchpad is implemented. If it\u2019s a [tool](https://www.anthropic.com/engineering/claude-think-tool?ref=blog.langchain.com), then an agent can simply read it by making a tool call. If it\u2019s part of the agent\u2019s runtime state, then the developer can choose what parts of state to expose to an agent each step. This provides a fine-grained level of control for exposing scratchpad context to the LLM at later turns.\n\n**Memories**\n\nIf agents have the ability to save memories, they also need the ability to select memories relevant to the task they are performing. This can be useful for a few reasons. Agents might select few-shot examples ([episodic](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#memory-types) [memories](https://arxiv.org/pdf/2309.02427?ref=blog.langchain.com)) for examples of desired behavior, instructions ([procedural](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#memory-types) [memories](https://arxiv.org/pdf/2309.02427?ref=blog.langchain.com)) to steer behavior, or facts ([semantic](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#memory-types) [memories](https://arxiv.org/pdf/2309.02427?ref=blog.langchain.com)) for task-relevant context.\n\n!One challenge is ensuring that relevant memories are selected. Some popular agents simply use a narrow set of files that are *always* pulled into context. For example, many code agent use specific files to save instructions (\u201dprocedural\u201d memories) or, in some cases, examples (\u201depisodic\u201d memories). Claude Code uses [`CLAUDE.md`](http://claude.md/?ref=blog.langchain.com). [Cursor](https://docs.cursor.com/context/rules?ref=blog.langchain.com) and [Windsurf](https://windsurf.com/editor/directory?ref=blog.langchain.com) use rules files. \n\nBut, if an agent is storing a larger [collection](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#collection) of facts and / or relationships (e.g., [semantic](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#memory-types) memories), selection is harder. [ChatGPT](https://help.openai.com/en/articles/8590148-memory-faq?ref=blog.langchain.com) is a good example of a popular product that stores and selects from a large collection of user-specific memories.\n\nEmbeddings and / or [knowledge](https://arxiv.org/html/2501.13956v1?ref=blog.langchain.com#:~:text=In%20Zep%2C%20memory%20is%20powered,subgraph%2C%20and%20a%20community%20subgraph) [graphs](https://neo4j.com/blog/developer/graphiti-knowledge-graph-memory/?ref=blog.langchain.com#:~:text=changes%20since%20updates%20can%20trigger,and%20holistic%20memory%20for%20agentic) for memory indexing are commonly used to assist with selection. Still, memory selection is challenging. At the AIEngineer World\u2019s Fair, [Simon Willison\u00a0shared](https://simonwillison.net/2025/Jun/6/six-months-in-llms/?ref=blog.langchain.com) an example of selection gone wrong: ChatGPT fetched his location from memories and unexpectedly injected it into a requested image. This type of unexpected or undesired memory retrieval can make some users feel like the context window \u201c*no longer belongs to them*\u201d! \n\n**Tools**\n\nAgents use tools, but can become overloaded if they are provided with too many. This is often because the tool descriptions overlap, causing model confusion about which tool to use. One approach is [to apply RAG (retrieval augmented generation) to tool descriptions](https://arxiv.org/abs/2410.14594?ref=blog.langchain.com) in order to fetch only the most relevant tools for a task. Some [recent papers](https://arxiv.org/abs/2505.03275?ref=blog.langchain.com) have shown that this improve tool selection accuracy by 3-fold.\n\n**Knowledge**\n\n[RAG](https://github.com/langchain-ai/rag-from-scratch?ref=blog.langchain.com) is a rich topic and it [can be a central context engineering challenge](https://x.com/_mohansolo/status/1899630246862966837?ref=blog.langchain.com). Code agents are some of the best examples of RAG in large-scale production. Varun from Windsurf captures some of these challenges well:\n\n> *Indexing code \u2260 context retrieval \u2026 [We are doing indexing & embedding search \u2026 [with] AST parsing code and chunking along semantically meaningful boundaries \u2026 embedding search becomes unreliable as a retrieval heuristic as the size of the codebase grows \u2026 we must rely on a combination of techniques like grep/file search, knowledge graph based retrieval, and \u2026 a re-ranking step where [context] is ranked in order of relevance.*\n\n### Compressing Context\n\n*Compressing context involves retaining only the tokens required to perform a task.*\n\n**Context Summarization**\n\nAgent interactions can span [hundreds of turns](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com) and use token-heavy tool calls. Summarization is one common way to manage these challenges. If you\u2019ve used Claude Code, you\u2019ve seen this in action. Claude Code runs \u201c[auto-compact](https://docs.anthropic.com/en/docs/claude-code/costs?ref=blog.langchain.com)\u201d after you exceed 95% of the context window and it will summarize the full trajectory of user-agent interactions. This type of compression across an [agent trajectory](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#manage-short-term-memory) can use various strategies such as [recursive](https://arxiv.org/pdf/2308.15022?ref=blog.langchain.com#:~:text=the%20retrieved%20utterances%20capture%20the,based%203) or [hierarchical](https://alignment.anthropic.com/2025/summarization-for-monitoring/?ref=blog.langchain.com#:~:text=We%20addressed%20these%20issues%20by,of%20our%20computer%20use%20capability) summarization.\n\n!A few places where summarization can be appliedIt can also be useful to [add summarization](https://github.com/langchain-ai/open_deep_research/blob/e5a5160a398a3699857d00d8569cb7fd0ac48a4f/src/open_deep_research/utils.py?ref=blog.langchain.com#L1407) at specific points in an agent\u2019s design. For example, it can be used to post-process certain tool calls (e.g., token-heavy search tools). As a second example, [Cognition](https://cognition.ai/blog/dont-build-multi-agents?ref=blog.langchain.com#a-theory-of-building-long-running-agents) mentioned summarization at agent-agent boundaries to reduce tokens during knowledge hand-off. Summarization can be a challenge if specific events or decisions need to be captured. [Cognition](https://cognition.ai/blog/dont-build-multi-agents?ref=blog.langchain.com#a-theory-of-building-long-running-agents) uses a fine-tuned model for this, which underscores how much work can go into this step.\n\n**Context Trimming**\n\nWhereas summarization typically uses an LLM to distill the most relevant pieces of context, trimming can often filter or, as Drew Breunig points out, \u201c[prune](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html?ref=blog.langchain.com)\u201d context. This can use hard-coded heuristics like removing [older messages](https://python.langchain.com/docs/how_to/trim_messages/?ref=blog.langchain.com) from a list. Drew also mentions [Provence](https://arxiv.org/abs/2501.16214?ref=blog.langchain.com), a trained context pruner for Question-Answering.\n\n### Isolating Context\n\n*Isolating context involves splitting it up to help an agent perform a task.*\n\n**Multi-agent**\n\nOne of the most popular ways to isolate context is to split it across sub-agents. A motivation for the OpenAI [Swarm](https://github.com/openai/swarm?ref=blog.langchain.com) library was [separation of concerns](https://openai.github.io/openai-agents-python/ref/agent/?ref=blog.langchain.com), where a team of agents can handle specific sub-tasks. Each agent has a specific set of tools, instructions, and its own context window.\n\n!Split context across multiple agentsAnthropic\u2019s [multi-agent researcher](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com) makes a case for this: many agents with isolated contexts outperformed single-agent, largely because each subagent context window can be allocated to a more narrow sub-task. As the blog said:\n\n> *[Subagents operate] in parallel with their own context windows, exploring different aspects of the question simultaneously.*\n\nOf course, the challenges with multi-agent include token use (e.g., up to [15\u00d7 more tokens](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com) than chat as reported by Anthropic), the need for careful [prompt engineering](https://www.anthropic.com/engineering/built-multi-agent-research-system?ref=blog.langchain.com) to plan sub-agent work, and coordination of sub-agents.\n\n**Context Isolation with Environments**\n\nHuggingFace\u2019s [deep researcher](https://huggingface.co/blog/open-deep-research?ref=blog.langchain.com#:~:text=From%20building%20,it%20can%20still%20use%20it) shows another interesting example of context isolation. Most agents use [tool calling APIs](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview?ref=blog.langchain.com), which return JSON objects (tool arguments) that can be passed to tools (e.g., a search API) to get tool feedback (e.g., search results). HuggingFace uses a [CodeAgent](https://huggingface.co/papers/2402.01030?ref=blog.langchain.com), which outputs that contains the desired tool calls. The code then runs in a [sandbox](https://e2b.dev/?ref=blog.langchain.com). Selected context (e.g., return values) from the tool calls is then passed back to the LLM.\n\n!Sandboxes can isolate context from the LLM.This allows context to be isolated from the LLM in the environment. Hugging Face noted that this is a great way to isolate token-heavy objects in particular:\n\n> *[Code Agents allow for] a better handling of state \u2026 Need to store this image / audio / other for later use? No problem, just assign it as a variable* [*in your state and you [use it later]*](https://deepwiki.com/search/i-am-wondering-if-state-that-i_0e153539-282a-437c-b2b0-d2d68e51b873?ref=blog.langchain.com)*.*\n\n**State**\n\nIt\u2019s worth calling out that an agent\u2019s runtime [state object](https://langchain-ai.github.io/langgraph/concepts/low_level/?ref=blog.langchain.com#state) can also be a great way to isolate context. This can serve the same purpose as sandboxing. A state object can be designed with a [schema](https://langchain-ai.github.io/langgraph/concepts/low_level/?ref=blog.langchain.com#schema) that has fields that context can be written to. One field of the schema (e.g., `messages`) can be exposed to the LLM at each turn of the agent, but the schema can isolate information in other fields for more selective use.\n\n### Context Engineering with LangSmith / LangGraph\n\nSo, how can you apply these ideas? Before you start, there are two foundational pieces that are helpful. First, ensure that you have a way to [look at your data](https://hamel.dev/blog/posts/evals/?ref=blog.langchain.com) and track token-usage across your agent. This helps inform where best to apply effort context engineering. [LangSmith](https://docs.smith.langchain.com/?ref=blog.langchain.com) is well-suited for agent [tracing / observability](https://docs.smith.langchain.com/observability?ref=blog.langchain.com), and offers a great way to do this. Second, be sure you have a simple way to test whether context engineering hurts or improve agent performance. LangSmith enables [agent evaluation](https://docs.smith.langchain.com/evaluation/tutorials/agents?ref=blog.langchain.com) to test the impact of any context engineering effort.\n\n**Write context**\n\nLangGraph was designed with both thread-scoped ([short-term](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#short-term-memory)) and [long-term memory](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#long-term-memory). Short-term memory uses [checkpointing](https://langchain-ai.github.io/langgraph/concepts/persistence/?ref=blog.langchain.com) to persist [agent state](https://langchain-ai.github.io/langgraph/concepts/low_level/?ref=blog.langchain.com#state) across all steps of an agent. This is extremely useful as a \u201cscratchpad\u201d, allowing you to write information to state and fetch it at any step in your agent trajectory.\n\nLangGraph\u2019s long-term memory lets you to persist context *across many sessions* with your agent. It is flexible, allowing you to save small sets of [files](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#profile) (e.g., a user profile or rules) or larger [collections](https://langchain-ai.github.io/langgraph/concepts/memory/?ref=blog.langchain.com#collection) of memories. In addition, [LangMem](https://langchain-ai.github.io/langmem/?ref=blog.langchain.com) provides a broad set of useful abstractions to aid with LangGraph memory management.\n\n**Select context**\n\nWithin each node (step) of a LangGraph agent, you can fetch [state](https://langchain-ai.github.io/langgraph/concepts/low_level/?ref=blog.langchain.com#state). This give you fine-grained control over what context you present to the LLM at each agent step. \n\nIn addition, LangGraph\u2019s long-term memory is accessible within each node and supports various types of retrieval (e.g., fetching files as well as [embedding-based retrieval on a memory collection).](https://langchain-ai.github.io/langgraph/cloud/reference/cli/?ref=blog.langchain.com#adding-semantic-search-to-the-store) For an overview of long-term memory, see [our Deeplearning.ai course](https://www.deeplearning.ai/short-courses/long-term-agentic-memory-with-langgraph/?ref=blog.langchain.com). And for an entry point to memory applied to a specific agent, see our [Ambient Agents](https://academy.langchain.com/courses/ambient-agents?ref=blog.langchain.com) course. This shows how to use LangGraph memory in a long-running agent that can manage your email and learn from your feedback.\n\n!Email agent with user feedback and long-term memoryFor tool selection, the [LangGraph Bigtool](https://github.com/langchain-ai/langgraph-bigtool?ref=blog.langchain.com) library is a great way to apply semantic search over tool descriptions. This helps select the most relevant tools for a task when working with a large collection of tools. Finally, we have several [tutorials and videos](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/?ref=blog.langchain.com) that show how to use various types of RAG with LangGraph.\n\n**Compressing context**\n\nBecause LangGraph [is a low-level orchestration framework](https://blog.langchain.com/how-to-think-about-agent-frameworks/), you [lay out your agent as a set of nodes](https://www.youtube.com/watch?v=aHCDrAbH_go&ref=blog.langchain.com), [define](https://blog.langchain.com/how-to-think-about-agent-frameworks/) the logic within each one, and define an state object that is passed between them. This control offers several ways to compress context.\n\nOne common approach is to use a message list as your agent state and [summarize or trim](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/?ref=blog.langchain.com#manage-short-term-memory) it periodically using [a few built-in utilities](https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/?ref=blog.langchain.com#manage-short-term-memory). However, you can also add logic to post-process [tool calls](https://github.com/langchain-ai/open_deep_research/blob/e5a5160a398a3699857d00d8569cb7fd0ac48a4f/src/open_deep_research/utils.py?ref=blog.langchain.com#L1407) or work phases of your agent in a few different ways. You can add summarization nodes at specific points or also add summarization logic to your tool calling node in order to compress the output of specific tool calls.\n\n**Isolating context**\n\nLangGraph is designed around a [state](https://langchain-ai.github.io/langgraph/concepts/low_level/?ref=blog.langchain.com#state) object, allowing you to specify a state schema and access state at each agent step. For example, you can store context from tool calls in certain fields in state, isolating them from the LLM until that context is required. In addition to state, LangGraph supports use of sandboxes for context isolation. See this [repo](https://github.com/jacoblee93/mini-chat-langchain?tab=readme-ov-file&ref=blog.langchain.com) for an example LangGraph agent that uses [an E2B sandbox](https://e2b.dev/?ref=blog.langchain.com) for tool calls. See this [video](https://www.youtube.com/watch?v=FBnER2sxt0w&ref=blog.langchain.com) for an example of sandboxing using Pyodide where state can be persisted. LangGraph also has a lot of support for building multi-agent architecture, such as the [supervisor](https://github.com/langchain-ai/langgraph-supervisor-py?ref=blog.langchain.com) and [swarm](https://github.com/langchain-ai/langgraph-swarm-py?ref=blog.langchain.com) libraries. You can [see](https://www.youtube.com/watch?v=4nZl32FwU-o&ref=blog.langchain.com) [these](https://www.youtube.com/watch?v=JeyDrn1dSUQ&ref=blog.langchain.com) [videos](https://www.youtube.com/watch?v=B_0TNuYi56w&ref=blog.langchain.com) for more detail on using multi-agent with LangGraph.\n\n### Conclusion\n\nContext engineering is becoming a craft that agents builders should aim to master. Here, we covered a few common patterns seen across many popular agents today:\n\n* *Writing context - saving it outside the context window to help an agent perform a task.*\n* *Selecting context - pulling it into the context window to help an agent perform a task.*\n* *Compressing context - retaining only the tokens required to perform a task.*\n* *Isolating context - splitting it up to help an agent perform a task.*\n\nLangGraph makes it easy to implement each of them and LangSmith provides an easy way to test your agent and track context usage. Together, LangGraph and LangGraph enable a virtuous feedback loop for identifying the best opportunity to apply context engineering, implementing it, testing it, and repeating.\n\n### Join our newsletter\n\nUpdates from the LangChain team and community\n\nEnter your email\n\nSubscribe\n\nProcessing your application...\n\nSuccess! Please check your inbox and click the link to confirm your subscription.\n\nSorry, something went wrong. Please try again.\n\n## Referenced Articles and Links\n\nThe following articles and resources were referenced in this content:\n\n### 1. Search code, repositories, users, issues, pull requests...\n**Source:** https://github.com/langchain-ai/langgraph-swarm-py?ref=blog.langchain.com\n\n**Summary:** # \ud83e\udd16 LangGraph Multi-Agent Swarm\n\nA Python library for creating swarm-style multi-agent systems using [LangGraph](https://github.com/langchain-ai/langgraph). A swarm is a type of [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent) architecture where agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent.\n\n[![Swarm...\n\n---\n\n### 2. Search code, repositories, users, issues, pull requests...\n**Source:** https://github.com/langchain-ai/langgraph-supervisor-py?ref=blog.langchain.com\n\n**Summary:** # \ud83e\udd16 LangGraph Multi-Agent Supervisor\n\nA Python library for creating hierarchical multi-agent systems using [LangGraph](https://github.com/langchain-ai/langgraph). Hierarchical systems are a type of [multi-agent](https://langchain-ai.github.io/langgraph/concepts/multi_agent) architecture where specialized agents are coordinated by a central **supervisor** agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current co...\n\n---\n\n### 3. Search code, repositories, users, issues, pull requests...\n**Source:** https://github.com/jacoblee93/mini-chat-langchain?tab=readme-ov-file&ref=blog.langchain.com\n\n**Summary:** # \ud83d\udc76 Mini Chat LangChain\n\nA minimal agentic implementation of [Chat LangChain](https://chat.langchain.com/) that can answer questions about LangGraph. Uses a small model and no indexing or vectorstores, just LangGraph's [LLMS.txt](https://langchain-ai.github.io/langgraph/llms.txt) file!\n\n[!](/jacoblee93/mini-chat-langchain/blob/main/static/img/mini-chat-langchain-studio.png)\n\nTo improve the correctness of generated code, Mini Chat LangChain verifies the correctness of generated code via a typeche...\n\n---\n\n"
  },
  {
    "id": 22,
    "title": "Sumit (@reachsumit.com)",
    "url": "https://bsky.app/profile/reachsumit.com/post/3ltnsm55rq227",
    "content": "# FrugalRAG: Learning to retrieve and reason for multi-hop QA\n\n**Shared via Bluesky:** https://bsky.app/profile/reachsumit.com/post/3ltnsm55rq227\n**Original Paper:** https://arxiv.org/abs/2507.07634\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\nSumit\nreachsumit.com\ndid:plc:iakjx7d3ypfa5ycegrylvt2m\nFrugalRAG: Learning to retrieve and reason for multi-hop QA\n\nIntroduces a two-stage training framework that achieves competitive RAG performance while reducing retrieval costs by nearly half using only 1000 training examples.\n\n\ud83d\udcdd arxiv.org/abs/2507.07634\nhttps://arxiv.org/abs/2507.07634\n2025-07-11T02:33:55.642Z\n\n---\n\n# FrugalRAG: Learning to retrieve and reason for multi-hop QA\n\n**Authors:** Authors:Abhinav Java, Srivathsan Koundinyan, Nagarajan Natarajan, Amit Sharma\n\n**Subjects:** Computation and Language (cs.CL)\n\n## Abstract\n\nWe consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-of-thought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples).\n\n"
  },
  {
    "id": 23,
    "title": "arxiv cs.IR (@arxiv-cs-ir.bsky.social)",
    "url": "https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lto4qcwxly2j",
    "content": "# Measuring Hypothesis Testing Errors in the Evaluation of Retrieval Systems\n\n**Shared via Bluesky:** https://bsky.app/profile/arxiv-cs-ir.bsky.social/post/3lto4qcwxly2j\n**Original Paper:** https://arxiv.org/abs/2507.07924\n\n## Social Media Context\n\n**Bluesky Post Commentary:**\nPost\narxiv cs.IR\narxiv-cs-ir.bsky.social\ndid:plc:aq67jotdjmceysktj6nq6gqy\nJack McKechnie, Graham McDonald, Craig Macdonald\nMeasuring Hypothesis Testing Errors in the Evaluation of Retrieval Systems\nhttps://arxiv.org/abs/2507.07924\n2025-07-11T05:35:13.237Z\n\n---\n\n# Measuring Hypothesis Testing Errors in the Evaluation of Retrieval Systems\n\n**Authors:** Authors:Jack McKechnie, Graham McDonald, Craig Macdonald\n\n**Subjects:** Information Retrieval (cs.IR)\n\n## Abstract\n\nThe evaluation of Information Retrieval (IR) systems typically uses query-document pairs with corresponding human-labelled relevance assessments (qrels). These qrels are used to determine if one system is better than another based on average retrieval performance. Acquiring large volumes of human relevance assessments is expensive. Therefore, more efficient relevance assessment approaches have been proposed, necessitating comparisons between qrels to ascertain their efficacy. Discriminative power, i.e. the ability to correctly identify significant differences between systems, is important for drawing accurate conclusions on the robustness of qrels. Previous work has measured the proportion of pairs of systems that are identified as significantly different and has quantified Type I statistical errors. Type I errors lead to incorrect conclusions due to false positive significance tests. We argue that also identifying Type II errors (false negatives) is important as they lead science in the wrong direction. We quantify Type II errors and propose that balanced classification metrics, such as balanced accuracy, can be used to portray the discriminative power of qrels. We perform experiments using qrels generated using alternative relevance assessment methods to investigate measuring hypothesis testing errors in IR evaluation. We find that additional insights into the discriminative power of qrels can be gained by quantifying Type II errors, and that balanced classification metrics can be used to give an overall summary of discriminative power in one, easily comparable, number.\n\n"
  },
  {
    "id": 24,
    "title": "Scott McGrath (@smcgrath.phd)",
    "url": "https://bsky.app/profile/smcgrath.phd/post/3lthihzv6ak27",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/smcgrath.phd/post/3lthihzv6ak27\n\n## Post Content\n\nPost\nScott McGrath\nsmcgrath.phd\ndid:plc:lrzkd5exmxqrblbruvjieofj\nA new paper reveals that LLMs can be jailbroken by transforming targeted queries into complex prose with fabricated academic citations. \n\nThis \"InfoFlood\" method overwhelms safety filters by exploiting the model's reliance on superficial cues for toxicity. \n#MLSky\nhttps://www.404media.co/researchers-jailbreak-ai-by-flooding-it-with-bullshit-jargon/\n2025-07-08T14:16:44.442Z\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n- https://www.404media.co/researchers-jailbreak-ai-by-flooding-it-with-bullshit-jargon/\n\n"
  },
  {
    "id": 25,
    "title": "Maria Antoniak (@mariaa.bsky.social)",
    "url": "https://bsky.app/profile/mariaa.bsky.social/post/3lumkyphrok2f",
    "content": "# Bluesky Post Analysis\n\n**Original Bluesky Post:** https://bsky.app/profile/mariaa.bsky.social/post/3lumkyphrok2f\n\n## Post Content\n\nPost\nMaria Antoniak\nmariaa.bsky.social\ndid:plc:y32vdst3y7r4f4pkrveb7jhi\n\"Just Put a Human in the Loop? Investigating LLM-Assisted Annotation for Subjective Tasks\"\n\narxiv.org/abs/2507.15821\n2025-07-23T08:10:19.403Z\n\n## Embedded Links\n\n- https://bsky.social\n- https://atproto.com\n\n"
  }
]
