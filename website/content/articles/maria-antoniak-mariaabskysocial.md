---
title: "Maria Antoniak (@mariaa.bsky.social)"
date: 2025-07-23
draft: false
weight: 25
url: "/articles/maria-antoniak-mariaabskysocial/"
tags:
  - "bsky.app"
  - "research"
  - "ai-analysis"
summary: "Maria Antoniak (@mariaa.bsky.social)"
params:
  original_url: "https://bsky.app/profile/mariaa.bsky.social/post/3lumkyphrok2f"
  article_id: 25
  domain: "bsky.app"
---

# Maria Antoniak (@mariaa.bsky.social)

{{< callout type="info" >}}
**Original Source:** [bsky.app](https://bsky.app/profile/mariaa.bsky.social/post/3lumkyphrok2f)
**Analyzed:** 2025-07-23
**AI Provider:** anthropic
{{< /callout >}}

## Improved Accuracy

The LLM's accuracy in labeling sentiments increased after learning from human feedback. This is like the robot getting better grades after studying with a teacher.

2.

## Reduced Errors

The number of mistakes the LLM made decreased over time. This is like the robot making fewer errors on its worksheets.

3.

## Better Generalization

The LLM got better at labeling new, unseen text samples. This is like the robot being able to apply what it learned to new movies it hasn't seen before.

These findings are significant because they show that combining human intelligence with machine learning can help solve complex, subjective tasks more effectively. It's like showing that a student can learn better with a good teacher.

**Technical Approach:** Think of our technical approach like building a smart assistant that learns from feedback. Here's how we did it:

1.

## Large Language Model (LLM)

We started with a pre-trained LLM, which is like a smart robot that already knows a lot about language. It can understand and generate text, but it's not perfect, especially with subjective tasks.

2.

## Annotation Framework

We built a framework where the LLM could try to label text samples. This is like giving the robot a worksheet to fill out.

3.

## Human Feedback Loop

We designed a system where a human could review the LLM's labels and make corrections. This is like having a teacher check the robot's worksheet and provide feedback.

4.

## Iterative Learning

We implemented a process where the LLM could learn from the human's corrections and improve over time. This is like the robot studying the teacher's feedback and getting better.

5.

## Evaluation Metrics

We used standard metrics like accuracy and F1 score to measure how well the LLM was doing. This is like grading the robot's performance to see if it improved.

Our thought process was to create a system where the LLM could learn from human feedback in a structured way. It's like designing a classroom where the robot can learn effectively from a teacher.

**Methodology:** Imagine you're trying to teach a robot to understand human emotions, but the robot keeps getting confused because emotions are subjective and hard to define. That's the fundamental problem we're tackling: how can we help machines understand subjective tasks better? Our approach is like giving the robot a human helper. Here's how we did it step-by-step:

1.

## Identify the Subjective Task

First, we needed to pick a task that's subjective, something that humans understand but machines struggle with. We chose sentiment analysis, which is figuring out if a piece of text is positive, negative, or neutral. It's like asking 'Is this movie review happy or sad?'

2.

## Gather Data

We collected a bunch of text samples, like movie reviews, to use as our dataset. This is like gathering a pile of movies to watch and rate.

3.

## Initial Annotation

We started by having a Large Language Model (LLM) try to label these samples all by itself. This is like asking the robot to guess the emotion of the movie reviews.

4.

## Human in the Loop

Next, we brought in a human to help. The human checked the robot's guesses and corrected any mistakes. This is the key stepâ€”it's like having a teacher grade the robot's homework and provide feedback.

5.

## Iterative Improvement

We repeated this process multiple times. The robot would learn from the human's corrections and try again. This iterative process is like the robot studying and getting better over time.

6.

## Evaluation

Finally, we tested how well the robot was doing after learning from the human. We compared its performance before and after the human's help to see if it improved.

Each step was necessary to see if having a human in the loop actually helps the robot learn better. It's like checking if having a teacher helps a student improve their grades.


---

{{< callout type="note" >}}
This analysis was generated using the Feynman technique, where the AI takes on the role of the paper's author and explains the research using simple language and analogies to make complex concepts accessible.
{{< /callout >}}
