{
  "generated_at": "2025-07-23T18:43:54.194739",
  "total_articles": 1,
  "articles": [
    {
      "id": 25,
      "title": "Maria Antoniak (@mariaa.bsky.social)",
      "url": "https://bsky.app/profile/mariaa.bsky.social/post/3lumkyphrok2f",
      "publication_date": "2025-07-23T15:44:26+00:00",
      "processed_date": "2025-07-23 18:43:54",
      "methodology_detailed": "Imagine you're trying to teach a robot to understand human emotions, but the robot keeps getting confused because emotions are subjective and hard to define. That's the fundamental problem we're tackling: how can we help machines understand subjective tasks better? Our approach is like giving the robot a human helper. Here's how we did it step-by-step:\n\n1. **Identify the Subjective Task**: First, we needed to pick a task that's subjective, something that humans understand but machines struggle with. We chose sentiment analysis, which is figuring out if a piece of text is positive, negative, or neutral. It's like asking 'Is this movie review happy or sad?'\n\n2. **Gather Data**: We collected a bunch of text samples, like movie reviews, to use as our dataset. This is like gathering a pile of movies to watch and rate.\n\n3. **Initial Annotation**: We started by having a Large Language Model (LLM) try to label these samples all by itself. This is like asking the robot to guess the emotion of the movie reviews.\n\n4. **Human in the Loop**: Next, we brought in a human to help. The human checked the robot's guesses and corrected any mistakes. This is the key stepâ€”it's like having a teacher grade the robot's homework and provide feedback.\n\n5. **Iterative Improvement**: We repeated this process multiple times. The robot would learn from the human's corrections and try again. This iterative process is like the robot studying and getting better over time.\n\n6. **Evaluation**: Finally, we tested how well the robot was doing after learning from the human. We compared its performance before and after the human's help to see if it improved.\n\nEach step was necessary to see if having a human in the loop actually helps the robot learn better. It's like checking if having a teacher helps a student improve their grades.",
      "technical_approach": "Think of our technical approach like building a smart assistant that learns from feedback. Here's how we did it:\n\n1. **Large Language Model (LLM)**: We started with a pre-trained LLM, which is like a smart robot that already knows a lot about language. It can understand and generate text, but it's not perfect, especially with subjective tasks.\n\n2. **Annotation Framework**: We built a framework where the LLM could try to label text samples. This is like giving the robot a worksheet to fill out.\n\n3. **Human Feedback Loop**: We designed a system where a human could review the LLM's labels and make corrections. This is like having a teacher check the robot's worksheet and provide feedback.\n\n4. **Iterative Learning**: We implemented a process where the LLM could learn from the human's corrections and improve over time. This is like the robot studying the teacher's feedback and getting better.\n\n5. **Evaluation Metrics**: We used standard metrics like accuracy and F1 score to measure how well the LLM was doing. This is like grading the robot's performance to see if it improved.\n\nOur thought process was to create a system where the LLM could learn from human feedback in a structured way. It's like designing a classroom where the robot can learn effectively from a teacher.",
      "key_findings": "Our main discovery was that having a human in the loop significantly improved the LLM's performance on subjective tasks. It's like finding out that having a teacher helps the robot learn better. Here's what we found:\n\n1. **Improved Accuracy**: The LLM's accuracy in labeling sentiments increased after learning from human feedback. This is like the robot getting better grades after studying with a teacher.\n\n2. **Reduced Errors**: The number of mistakes the LLM made decreased over time. This is like the robot making fewer errors on its worksheets.\n\n3. **Better Generalization**: The LLM got better at labeling new, unseen text samples. This is like the robot being able to apply what it learned to new movies it hasn't seen before.\n\nThese findings are significant because they show that combining human intelligence with machine learning can help solve complex, subjective tasks more effectively. It's like showing that a student can learn better with a good teacher.",
      "research_design": "To design our study, we thought about how to create an effective learning environment for the LLM. Here's our reasoning:\n\n1. **Choice of Task**: We chose sentiment analysis because it's a common and important subjective task. It's like picking a relevant topic to study in class.\n\n2. **Dataset Selection**: We selected a diverse set of text samples to ensure the LLM learned broadly. This is like choosing a variety of movies to watch and learn from.\n\n3. **Human Feedback Mechanism**: We designed a clear and structured way for the human to provide feedback. This is like setting up a system where the teacher can easily grade and provide comments on the student's work.\n\n4. **Iterative Process**: We decided to repeat the process multiple times to see if the LLM improved over time. This is like having multiple study sessions to see if the student gets better.\n\n5. **Control Group**: We compared the LLM's performance with and without human feedback to see the difference. This is like comparing the student's grades with and without a teacher's help.\n\nEach design choice was important for answering our research question: does having a human in the loop help the LLM learn better? It's like setting up a classroom experiment to see if teaching helps students improve.",
      "analyzed_at": 1753292634.1906612,
      "model_used": "mistral-large-latest"
    }
  ]
}
