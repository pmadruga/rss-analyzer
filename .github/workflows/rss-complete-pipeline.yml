name: ðŸ”„ RSS Complete Pipeline

on:
  schedule:
    # Run daily at 8:00 AM UTC
    - cron: '0 8 * * *'
  workflow_dispatch:
    inputs:
      max_articles:
        description: 'Maximum number of articles to process'
        required: false
        default: '10'
        type: string
      test_only:
        description: 'Run in test mode (1 article only)'
        required: false
        default: false
        type: boolean

env:
  # API Configuration
  API_PROVIDER: 'mistral'
  MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  # Processing Configuration
  MAX_ARTICLES_PER_RUN: ${{ github.event.inputs.test_only == 'true' && '1' || github.event.inputs.max_articles || '10' }}
  FOLLOW_LINKS: ${{ github.event.inputs.test_only == 'true' && 'false' || 'true' }}

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: "rss-pipeline"
  cancel-in-progress: false

jobs:
  rss-complete-pipeline:
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
    # ========================================
    # SETUP PHASE
    # ========================================
    - name: ðŸ”„ Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: âš¡ Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: ðŸ“¦ Install dependencies
      run: |
        uv sync

    - name: ðŸ“ Create directories
      run: |
        mkdir -p data logs output docs

    - name: ðŸ—„ï¸ Initialize database WAL mode
      run: |
        echo "ðŸ—„ï¸ Setting up database with WAL mode for concurrency..."
        if [ -f "data/articles.db" ]; then
          echo "Database exists, ensuring WAL mode is enabled..."
          sqlite3 data/articles.db "PRAGMA journal_mode = WAL;"
          echo "âœ… Database configured with WAL mode"
        else
          echo "Database will be created with WAL mode on first use"
        fi

    # ========================================
    # TEST PHASE
    # ========================================
    - name: ðŸ”¬ Validate imports and dependencies
      run: |
        echo "ðŸ”¬ Validating all imports and dependencies..."
        uv run python tools/validate_imports.py

    - name: ðŸ” Test RSS parsing
      run: |
        echo "ðŸ” Testing RSS feed connectivity..."
        uv run python -c "
        from src.core.rss_parser import RSSParser
        parser = RSSParser()
        entries = parser.fetch_feed('https://bg.raindrop.io/rss/public/57118738')
        print(f'âœ… RSS parsing test: Found {len(entries)} entries')
        "

    - name: ðŸŒ Test web scraping
      run: |
        echo "ðŸŒ Testing web scraping..."
        uv run python -c "
        from src.core.scraper import WebScraper
        scraper = WebScraper()
        result = scraper.scrape_article('https://jina.ai/news/quantization-aware-training-of-jina-embeddings-v4/', follow_links=False)
        print(f'âœ… Scraper test: {\"Success\" if result else \"Failed\"}')
        if result:
            print(f'   Content length: {len(result.content)} characters')
        "

    - name: ðŸ—„ï¸ Test database operations
      run: |
        echo "ðŸ—„ï¸ Testing database..."
        uv run python -c "
        from src.core.database import DatabaseManager
        db = DatabaseManager('data/test.db')
        print('âœ… Database test: Initialized successfully')
        "

    - name: ðŸ§ª Run test analysis (1 article)
      if: github.event.inputs.test_only == 'true'
      run: |
        echo "ðŸ§ª Running test analysis with 1 article..."
        uv run python -m src.main run --limit 1 || true

    # ========================================
    # RSS SYNCHRONIZATION CHECK PHASE
    # ========================================
    - name: ðŸ” Check RSS feed synchronization
      if: github.event.inputs.test_only != 'true'
      run: |
        echo "ðŸ” Checking RSS feed synchronization..."
        chmod +x tools/ensure_rss_synced.sh
        tools/ensure_rss_synced.sh

    # ========================================
    # DEDUPLICATION PRE-PROCESS PHASE
    # ========================================
    - name: ðŸ” Pre-process duplicate check
      if: github.event.inputs.test_only != 'true'
      run: |
        echo "ðŸ” Checking for duplicates before processing..."

        # Check if database exists
        if [ -f "data/articles.db" ]; then
          echo "ðŸ“Š Running duplicate detection with statistics..."
          uv run python tools/check_duplicates.py --stats || true

          echo "âš¡ Building deduplication cache for O(1) lookups..."
          uv run python -c "
          from src.deduplication_manager import DeduplicationManager
          import time

          start_time = time.time()
          dedup = DeduplicationManager('data/articles.db')
          cache_size = dedup.build_cache()
          elapsed = time.time() - start_time

          print(f'âœ… Built deduplication cache: {cache_size} articles in {elapsed:.2f}s')
          print(f'ðŸš€ Ready for O(1) duplicate detection during processing')
          " || true
        else
          echo "â„¹ï¸ No existing database found, skipping duplicate check"
        fi

    # ========================================
    # ANALYSIS PHASE (Skip if test_only)
    # ========================================
    - name: ðŸ“Š Run full RSS analysis (backup)
      if: github.event.inputs.test_only != 'true'
      run: |
        echo "ðŸ“Š Running backup RSS analysis check..."
        set -e  # Exit on any error

        # Run sync check again to ensure completeness
        if uv run python tools/check_rss_sync.py; then
          echo "âœ… RSS sync verification passed"
        else
          echo "âš ï¸ RSS sync check indicates missing articles, running additional processing..."
          # Run additional analysis to catch any remaining articles
          uv run python -m src.main --log-level DEBUG run --limit 30 || true
        fi

        # Final verification and reporting
        if [ -f "data/articles.db" ]; then
          echo "ðŸ—„ï¸ Final database status:"
          uv run python -c "
          import sqlite3
          conn = sqlite3.connect('data/articles.db')
          cursor = conn.cursor()
          try:
              cursor.execute('SELECT COUNT(*) FROM articles')
              total = cursor.fetchone()[0]
              cursor.execute('SELECT status, COUNT(*) FROM articles GROUP BY status')
              status_counts = cursor.fetchall()
              print(f'Total articles in database: {total}')
              for status, count in status_counts:
                  print(f'  {status}: {count}')

              # Check completion rate
              cursor.execute('SELECT COUNT(*) FROM articles WHERE status = \"completed\"')
              completed = cursor.fetchone()[0]
              completion_rate = (completed / total * 100) if total > 0 else 0
              print(f'Completion rate: {completion_rate:.1f}%')
          except Exception as e:
              print(f'Error querying database: {e}')
          finally:
              conn.close()
          " 2>/dev/null || echo "Could not query database"
        fi

    - name: ðŸ“ Generate articles by date
      if: github.event.inputs.test_only != 'true'
      run: |
        echo "ðŸ“ Generating articles by date..."
        uv run python tools/generate_articles_by_date.py

    - name: ðŸ“Š Duplicate detection report
      if: github.event.inputs.test_only != 'true'
      run: |
        echo "ðŸ“Š Generating post-process duplicate detection report..."

        if [ -f "data/articles.db" ]; then
          echo "ðŸ” Final duplicate check after processing..."
          DUPLICATE_REPORT=$(uv run python tools/check_duplicates.py --report 2>&1)

          echo "$DUPLICATE_REPORT"

          # Extract key metrics
          DUPLICATE_COUNT=$(echo "$DUPLICATE_REPORT" | grep -oP 'Found \K\d+(?= duplicate)' || echo "0")
          TOTAL_ARTICLES=$(echo "$DUPLICATE_REPORT" | grep -oP 'Total articles: \K\d+' || echo "0")

          if [ "$DUPLICATE_COUNT" -gt "0" ]; then
            echo "âš ï¸ Warning: $DUPLICATE_COUNT duplicates detected in final database"
            echo "ðŸ’¡ Run deduplication-check workflow to clean up duplicates"
          else
            echo "âœ… No duplicates detected - database is clean"
          fi

          # Save report for artifacts
          echo "$DUPLICATE_REPORT" > output/final_duplicate_report.txt
        else
          echo "â„¹ï¸ No database found for duplicate reporting"
        fi

    - name: ðŸ“ˆ Generate analysis summary
      if: github.event.inputs.test_only != 'true'
      run: |
        echo "ðŸ“ˆ Generating analysis summary..."
        uv run python -c "
        import sqlite3
        import json
        from datetime import datetime
        import os

        if os.path.exists('data/articles.db'):
            conn = sqlite3.connect('data/articles.db')
            cursor = conn.cursor()

            cursor.execute('SELECT COUNT(*) FROM articles WHERE status = \"completed\"')
            completed = cursor.fetchone()[0]

            cursor.execute('SELECT COUNT(*) FROM articles')
            total = cursor.fetchone()[0]

            cursor.execute('SELECT MAX(processed_date) FROM articles WHERE status = \"completed\"')
            last_processed = cursor.fetchone()[0]

            conn.close()

            summary = {
                'run_date': datetime.now().isoformat(),
                'total_articles': total,
                'completed_articles': completed,
                'last_processed': last_processed
            }

            with open('output/run_summary.json', 'w') as f:
                json.dump(summary, f, indent=2)

            print(f'ðŸ“Š Analysis complete: {completed}/{total} articles processed')
        else:
            print('â„¹ï¸ No database found, skipping summary generation')
        "

    # ========================================
    # WEBSITE DATA UPDATE PHASE
    # ========================================
    - name: ðŸŒ Generate website data
      run: |
        echo "ðŸŒ Generating website data..."
        uv run python tools/generate_website_data.py --verbose || true

    - name: âœ… Validate website data
      run: |
        if [ -f "docs/data.json" ]; then
          echo "âœ… Validating generated JSON..."
          python -m json.tool docs/data.json > /dev/null
          echo "âœ… JSON validation successful"

          SIZE=$(stat -c%s docs/data.json 2>/dev/null || stat -f%z docs/data.json)
          echo "ðŸ“Š Generated data.json size: $SIZE bytes"
        else
          echo "âš ï¸ No data.json generated (no articles to process)"
        fi

    # ========================================
    # WEBSITE LAYOUT UPDATE PHASE
    # ========================================
    - name: ðŸŽ¨ Setup GitHub Pages
      uses: actions/configure-pages@v4

    - name: ðŸ“¤ Upload website files
      uses: actions/upload-pages-artifact@v3
      with:
        path: 'docs'

    - name: ðŸš€ Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4

    # ========================================
    # COMMIT CHANGES PHASE
    # ========================================
    - name: ðŸ’¾ Commit and push changes
      if: github.event.inputs.test_only != 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "RSS Complete Pipeline"

        # Add generated files
        git add docs/data.json output/ -f || true

        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "â„¹ï¸ No changes to commit"
        else
          # Create commit message
          if [ -f "output/run_summary.json" ]; then
            SUMMARY=$(cat output/run_summary.json | jq -r '"Articles: " + (.completed_articles | tostring) + "/" + (.total_articles | tostring)' 2>/dev/null || echo "Analysis completed")
          else
            SUMMARY="Website data updated"
          fi

          git commit -m "ðŸ”„ RSS Complete Pipeline - $(date -u '+%Y-%m-%d %H:%M UTC')

          $SUMMARY

          ðŸ¤– Generated with RSS Complete Pipeline

          Co-Authored-By: RSS-Pipeline-Bot <noreply@github.com>"

          git push
          echo "âœ… Changes committed and pushed"
        fi

    # ========================================
    # ARTIFACTS AND REPORTING PHASE
    # ========================================
    - name: ðŸ“¤ Upload database backup
      if: github.event.inputs.test_only != 'true'
      uses: actions/upload-artifact@v4
      with:
        name: database-backup-${{ github.run_number }}
        path: data/articles.db
        retention-days: 30
        if-no-files-found: ignore

    - name: ðŸ“¤ Upload logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: pipeline-logs-${{ github.run_number }}
        path: |
          logs/
          output/
        retention-days: 7
        if-no-files-found: ignore

    - name: ðŸ“Š Generate pipeline summary
      if: always()
      run: |
        if [ "${{ job.status }}" = "success" ]; then
          STATUS="âœ… RSS Complete Pipeline completed successfully"
          EMOJI="ðŸŸ¢"
        else
          STATUS="âŒ RSS Complete Pipeline failed"
          EMOJI="ðŸ”´"
        fi

        echo "## $EMOJI RSS Complete Pipeline Status" >> $GITHUB_STEP_SUMMARY
        echo "$STATUS" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f "output/run_summary.json" ]; then
          SUMMARY=$(cat output/run_summary.json | jq -r '"Articles: " + (.completed_articles | tostring) + "/" + (.total_articles | tostring)' 2>/dev/null || echo "Analysis data unavailable")
          echo "**Summary:** $SUMMARY" >> $GITHUB_STEP_SUMMARY
        fi

        echo "**Test Mode:** ${{ github.event.inputs.test_only || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "**Pipeline run:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY

        if [ "${{ steps.deployment.outputs.page_url }}" ]; then
          echo "**Website:** ${{ steps.deployment.outputs.page_url }}" >> $GITHUB_STEP_SUMMARY
        fi
